{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec8bacf7-7929-4c25-a072-57078a9781a6"
      },
      "source": [
        "\n",
        "<h1 align=center><font size = 5>Convolutional Neural Networks with Keras</font></h1>\n",
        "\n",
        "<h1 align=center><font size = 5>Muhammad Zeeshan</font></h1>\n",
        "\n",
        "<h1 align=center><font size = 5>Machine learning Engineer</font></h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "076c4e44-6f8c-4e98-b201-27e8cc35427c"
      },
      "source": [
        "In this lab, we will learn how to use the Keras library to build convolutional neural networks. We will also use the popular MNIST dataset and we will compare our results to using a conventional neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0af36d5-c39c-4db1-9abf-e6ae24b6d429"
      },
      "source": [
        "<h2>Convolutional Neural Networks with Keras</h2>\n",
        "\n",
        "<h3>Objective for this Notebook<h3>    \n",
        "<h5> 1. How to use the Keras library to build convolutional neural networks.</h5>\n",
        "<h5> 2. Convolutional Neural Network with One Convolutional and Pooling Layers.</h5>\n",
        "<h5> 3. Convolutional Neural Network with Two Convolutional and Pooling Layers.</h5>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a145693f-77be-4698-9305-933d6541bed9"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>\n",
        "      \n",
        "1. <a href=\"#item41\">Import Keras and Packages</a>   \n",
        "2. <a href=\"#item42\">Convolutional Neural Network with One Convolutional and Pooling Layers</a>  \n",
        "3. <a href=\"#item43\">Convolutional Neural Network with Two Convolutional and Pooling Layers</a>  \n",
        "\n",
        "</font>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9f28727-1324-4544-86e8-5dc44e72cc9e"
      },
      "source": [
        "<a id='item41'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd1a9901-1ad1-44e2-8d7c-6abbc1d4c5fa"
      },
      "source": [
        "## Import Keras and Packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4daa2c1-9de3-406c-8e5b-192095c7899a"
      },
      "source": [
        "Let's start by importing the keras libraries and the packages that we would need to build a neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7f754a0a-5333-4ac6-b3db-8571b16421c7",
        "outputId": "acbb341e-5e24-4e78-e680-2c0cddcd4769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.21.4\n",
            "  Using cached numpy-1.21.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.1\n",
            "    Uninstalling numpy-1.26.1:\n",
            "      Successfully uninstalled numpy-1.26.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "flax 0.7.5 requires numpy>=1.22, but you have numpy 1.21.4 which is incompatible.\n",
            "jax 0.4.20 requires numpy>=1.22, but you have numpy 1.21.4 which is incompatible.\n",
            "jaxlib 0.4.20+cuda11.cudnn86 requires numpy>=1.22, but you have numpy 1.21.4 which is incompatible.\n",
            "mizani 0.9.3 requires pandas>=1.3.5, but you have pandas 1.3.4 which is incompatible.\n",
            "numba 0.58.1 requires numpy<1.27,>=1.22, but you have numpy 1.21.4 which is incompatible.\n",
            "plotnine 0.12.4 requires numpy>=1.23.0, but you have numpy 1.21.4 which is incompatible.\n",
            "plotnine 0.12.4 requires pandas>=1.5.0, but you have pandas 1.3.4 which is incompatible.\n",
            "scipy 1.11.3 requires numpy<1.28.0,>=1.21.6, but you have numpy 1.21.4 which is incompatible.\n",
            "tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.21.4 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.21.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas==1.3.4 in /usr/local/lib/python3.10/dist-packages (1.3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.4) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.4) (1.21.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.4) (1.16.0)\n",
            "Collecting keras==2.1.6\n",
            "  Using cached Keras-2.1.6-py2.py3-none-any.whl (339 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras==2.1.6) (1.21.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.10/dist-packages (from keras==2.1.6) (1.11.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras==2.1.6) (1.16.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from keras==2.1.6) (6.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras==2.1.6) (3.9.0)\n",
            "Collecting numpy>=1.9.1 (from keras==2.1.6)\n",
            "  Using cached numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Installing collected packages: numpy, keras\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.4\n",
            "    Uninstalling numpy-1.21.4:\n",
            "      Successfully uninstalled numpy-1.21.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.14.0\n",
            "    Uninstalling keras-2.14.0:\n",
            "      Successfully uninstalled keras-2.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.1 which is incompatible.\n",
            "mizani 0.9.3 requires pandas>=1.3.5, but you have pandas 1.3.4 which is incompatible.\n",
            "plotnine 0.12.4 requires pandas>=1.5.0, but you have pandas 1.3.4 which is incompatible.\n",
            "tensorflow 2.14.0 requires keras<2.15,>=2.14.0, but you have keras 2.1.6 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.1.6 numpy-1.26.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
        "# If you run this notebook on a different environment, e.g. your desktop, you may need to uncomment and install certain libraries.\n",
        "\n",
        "!pip install numpy==1.21.4\n",
        "!pip install pandas==1.3.4\n",
        "!pip install keras==2.1.6"
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        },
        "id": "gx1PMJxLT5GK",
        "outputId": "2a42ba58-6149-42d3-cb93-c46e343479f6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Collecting keras<2.15,>=2.14.0 (from tensorflow)\n",
            "  Using cached keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: Keras 2.1.6\n",
            "    Uninstalling Keras-2.1.6:\n",
            "      Successfully uninstalled Keras-2.1.6\n",
            "Successfully installed keras-2.14.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caf13643-fbbb-4805-bca7-e7058b620f23"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dbe4f53-8aba-4207-a65b-cc86be6ab692"
      },
      "source": [
        "When working with convolutional neural networks in particular, we will need additional packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "76703d9c-3ebb-4120-9aa5-64e7b56f6df9",
        "outputId": "a431b2d6-48fe-4bf6-c64e-cc53f27194d9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-2d6e161c9ce4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m \u001b[0;31m# to add convolutional layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaxPooling2D\u001b[0m \u001b[0;31m# to add pooling layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlatten\u001b[0m \u001b[0;31m# to flatten data for fully connected layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.convolutional'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from keras.layers.convolutional import Conv2D # to add convolutional layers\n",
        "from keras.layers.convolutional import MaxPooling2D # to add pooling layers\n",
        "from keras.layers import Flatten # to flatten data for fully connected layers"
      ],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28bec1ed-fae8-4523-9400-03f259b83118"
      },
      "source": [
        "<a id='item42'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c2c6d4d-4199-4ac3-86fd-83d0af27b76b"
      },
      "source": [
        "## Convolutional Layer with One set of convolutional and pooling layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b926053-ed83-4419-9df6-bbafcfc7f2e2"
      },
      "outputs": [],
      "source": [
        "# import data\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# reshape to be [samples][pixels][width][height]\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75bfc804-4b7e-4ce6-b5e8-3ddab966108d"
      },
      "source": [
        "Let's normalize the pixel values to be between 0 and 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cd56d94-762f-4a2b-8116-6f04313c5228"
      },
      "outputs": [],
      "source": [
        "X_train = X_train / 255 # normalize training data\n",
        "X_test = X_test / 255 # normalize test data"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e10086d-2c0e-481e-b136-47ce17d7bd47"
      },
      "source": [
        "Next, let's convert the target variable into binary categories\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a368493-3d57-47f8-a735-7807ba675e3d"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "num_classes = y_test.shape[1] # number of categories"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31a494f7-8bb2-4971-a5b7-a8d17b391958"
      },
      "source": [
        "Next, let's define a function that creates our model. Let's start with one set of convolutional and pooling layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "762b15ce-04b4-4b52-91f4-5531b095adc2"
      },
      "outputs": [],
      "source": [
        "def convolutional_model():\n",
        "\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(16, (5, 5), strides=(1, 1), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adcd9352-8259-49e2-ac90-810ed08838f4"
      },
      "source": [
        "Finally, let's call the function to create the model, and then let's train it and evaluate it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bc53a31-62d6-4bbc-b622-2c22b7e45c9f"
      },
      "outputs": [],
      "source": [
        "# build the model\n",
        "model = convolutional_model()\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: {} \\n Error: {}\".format(scores[1], 100-scores[1]*100))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fda50d4-2b28-4a03-95d3-a78f6e289992"
      },
      "source": [
        "------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b4bcd75-801e-4ccb-b268-a35d3249d39c"
      },
      "source": [
        "<a id='item43'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dacc665a-cf08-4b37-93e9-51b24a3c5f65"
      },
      "source": [
        "## Convolutional Layer with two sets of convolutional and pooling layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89123c1f-18ba-402b-8d14-83ed04bac39a"
      },
      "source": [
        "Let's redefine our convolutional model so that it has two convolutional and pooling layers instead of just one layer of each.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4b3fb97-94a5-421b-80c1-8c4fe4134eb2"
      },
      "outputs": [],
      "source": [
        "def convolutional_model():\n",
        "\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(16, (5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(8, (2, 2), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90e1050d-e6b1-4a7e-a028-273b119dcc68"
      },
      "source": [
        "Now, let's call the function to create our new convolutional neural network, and then let's train it and evaluate it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c9020f2-a385-4ba5-9923-7fa42be5dd32"
      },
      "outputs": [],
      "source": [
        "# build the model\n",
        "model = convolutional_model()\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: {} \\n Error: {}\".format(scores[1], 100-scores[1]*100))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b5916c4-33c0-4f2b-8592-f6e030e82837"
      },
      "source": [
        "### Thank you for completing this lab!\n",
        "\n",
        "This notebook was created by [Muhammad Zeeshan](https://www.linkedin.com/in/muhammadzeeshan007/). I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d75c5eb1-532d-480c-947b-c161c7125c7e"
      },
      "source": [
        "\n",
        "## Change Log\n",
        "\n",
        "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
        "|---|---|---|---|\n",
        "| 2023-11-11  | 2.0  | Muhammad Zeeshan  |  XBOOM AI Lab to Markdown and added to course repo in GitLab |\n",
        "\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\"> © XBOOM AI Corporation 2023. All rights reserved. <h3/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fed37486-39c4-4a8d-ad63-6cce83855312"
      },
      "source": [
        "This notebook is part of a course on **XBOOM AI** called *Introduction to Deep Learning & Neural Networks with Keras*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://www.linkedin.com/in/muhammadzeeshan007/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da0bf7af-6309-43e4-bf82-b77f14d5de2c"
      },
      "source": [
        "<hr>\n",
        "\n",
        "Copyright &copy; 2019 [ XBOOM AI Developer Skills Network](https://www.linkedin.com/in/muhammadzeeshan007/). This notebook and its source code are released under the terms of the [MIT License](https://www.linkedin.com/in/muhammadzeeshan007/).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S7bre_FFgvh2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "name": ""
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}